{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING (Part1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get familiar with your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any calculation, just have a look at your data. Even with cured data sets, the answer to one or more of the following questions could be \"yes\":\n",
    "- Is there anything wrong with the data?\n",
    "- Are there any quirks with the data?\n",
    "- Do I need to fix or remove any of the data?\n",
    "\n",
    "Let's read some data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "init_data = pd.read_csv('init_data.csv')\n",
    "init_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visual inspection (we see the first 5 and the last 5), we can detect some possible quirks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- v5 seems to be constant\n",
    "- v6 seems to be -v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to confirm the suspicions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed\n",
    "- v5 has null std, thus constant. We remove it.\n",
    "- v6 and v1 have opposite mean, the same std, and opposite-crossed min, max and quartiles. We check whether v6 is equal to -v1. If so, we remove v6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(init_data.v6.values) == list(init_data.v1.values * (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data.drop('v5', axis=1, inplace=True)\n",
    "init_data.drop('v6', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something else? What about v4 and v7? \n",
    "A picture is worth a thousand words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.pairplot(init_data, hue='c', vars=['v1', 'v2', 'v3', 'v4', 'v7'], height=2.5, palette=None);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship with v1, v2, and v3 of both v4 and v7 is the same, but for the scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that v7 is equal to v4+2.5. Let's check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(init_data.v7.values) == list(init_data.v4.values + 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove v7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data.drop('v7', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set, as it is now, is a famous data set used by Fisher in 1936, called Iris.\n",
    "The variables v1 to v4 correspond to measurements of sepal length, sepal width, petal length, and petal width of three species of Iris plants (Iris Setosa, Iris Versicolor, and Iris Virginica; corresponding to 1, 2 and 3 in our variable c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = init_data\n",
    "iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "iris.species.replace([1, 2, 3], ['Setosa', 'Versicolor', 'Virginica'], inplace=True)\n",
    "sns.pairplot(iris, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], height=2.5, palette=None);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is coceived for trying to determine the species of an Iris plant, using the four measurements. Therefore, it is a classification problem with three classes, corresponding to the three species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Setosa is separable from the rest just by looking at petal lengths or widths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, it does not seem obvious for Versicolor and Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will have a look at outliers and missing values. We consider the following (naÃ¯ve) approaches:\n",
    "- Outliers: Remove rows containing outliers. We will consider both individual and colective  outlier detections.\n",
    "- Missing values (nan): Imputation using the mean value, or row removal. \"nan\" stands for \"not a number\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n",
    "- Outliers + nan? => Removing a row because of an outlier affects the mean of all columns, not only the one containing it, for the subsequent nan treatment. We assume that nan are ignored in the mean and std initial calculation.\n",
    "- nan + outliers? => The imputation using the mean does not affect the posterior mean, but affects the std.\n",
    "\n",
    "Which one would you perform first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "[your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens in our Iris data in both outliers and missing values treatments separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have artificially introduced some nan in petal length and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_data = pd.read_csv('nan_data.csv')\n",
    "nan_data.species.replace(['Setosa', 'Versicolor', 'Virginica'], [1, 2, 3], inplace=True)\n",
    "nan_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the count for petal length and width we can see that there are 7 nan in petal length and 6 nan in petal width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "nan_imputed_data = pd.DataFrame(data=imp.fit_transform(nan_data))\n",
    "nan_imputed_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "nan_imputed_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the plots look now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_imputed_data.species.replace([1, 2, 3], ['Setosa', 'Versicolor', 'Virginica'], inplace=True)\n",
    "sns.pairplot(nan_imputed_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], height=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obvious separation is possible anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had just ignored those rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**\n",
    "How many rows will remain after the deletion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "[your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_removed_data = nan_data\n",
    "nan_removed_data.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether you were right or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_removed_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2b:** Were you right? What is the reason for that failure/success?\n",
    "\n",
    "#### Answer:\n",
    "[your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_removed_data.species.replace([1, 2, 3], ['Setosa', 'Versicolor', 'Virginica'], inplace=True)\n",
    "sns.pairplot(nan_removed_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], height=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the nan have been artificially introduced by us, we will not further use nan_removed_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check colectively, using Mahalanobis distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "outlier_data = pd.read_csv('iris_data.csv')\n",
    "outlier_data.species.replace(['Setosa', 'Versicolor', 'Virginica'], [1, 2, 3], inplace=True)\n",
    "elip_env = sklearn.covariance.EllipticEnvelope().fit(outlier_data)\n",
    "detection = elip_env.predict(outlier_data)\n",
    "outlier_positions_mah = [x for x in range(outlier_data.shape[0]) if detection[x] == -1]\n",
    "if detection is []:\n",
    "    print(\"There are not outliers in the data.\")\n",
    "else:\n",
    "    print(\"The \" + str(len(outlier_positions_mah)) + \" outliers found are in positions:\\n\" + str(outlier_positions_mah))\n",
    "    classes_names = ['Setosa', 'Versicolor', 'Virginica']\n",
    "    print(\"They correspond respectively to classes:\\n\" +\n",
    "          str([classes_names[x-1] for x in outlier_data.species.values[outlier_positions_mah]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.species.values[outlier_positions_mah] += 3\n",
    "outlier_data.species.replace([1, 2, 3, 4, 5, 6], \n",
    "                             ['Setosa', 'Versicolor', 'Virginica', 'Outliers Setosa',\n",
    "                              'Outliers Versicolor', 'Outliers Virginica'], inplace=True)\n",
    "sns.pairplot(outlier_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], height=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there is not a single variable contributing alone to the colective assertion as outliers of those 15 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question3:** Then, why are there outliers? Hint: Think on the shape of an ellipsoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "[your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check individualy, i.e. variable by variable. We opt for the robust option based on boxplots, i.e.\n",
    "$$x \\in X \\:\\: outlier \\:\\:\\Leftrightarrow \\:\\: x\\notin \\left[Q_1 - 1.5 * IQR, Q_3 + 1.5 * IQR\\right]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data = pd.read_csv('iris_data.csv')\n",
    "outlier_data.species.replace(['Setosa', 'Versicolor', 'Virginica'], [1, 2, 3], inplace=True)\n",
    "ax = sns.boxplot(data=outlier_data[outlier_data.columns[:-1]], orient=\"h\", palette=\"Set2\", linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closer look to sepal width:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = sns.boxplot(y=\"sepal_width\", data=outlier_data, orient=\"h\", color=sns.color_palette(\"Set2\")[1], linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR = outlier_data.describe()[\"sepal_width\"][\"75%\"] - outlier_data.describe()[\"sepal_width\"][\"25%\"]\n",
    "whiskers = [outlier_data.describe()[\"sepal_width\"][\"25%\"] - (1.5 * IQR), outlier_data.describe()[\"sepal_width\"][\"75%\"] + (1.5 * IQR)]\n",
    "outlier_positions_box = [x for x in range(outlier_data.shape[0]) if outlier_data.sepal_width.values[x] < whiskers[0] or outlier_data.sepal_width.values[x] > whiskers[1]]\n",
    "print(\"The outliers found are in positions:\\n\" + str(outlier_positions_box))\n",
    "print(\"They correspond respectively to sepal widths:\\n\" + str(outlier_data.sepal_width.values[outlier_positions_box]))\n",
    "classes_names = ['Setosa', 'Versicolor', 'Virginica']\n",
    "print(\"They correspond respectively to classes:\\n\" + str([classes_names[x-1] for x in outlier_data.species.values[outlier_positions_box]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.species.values[outlier_positions_box] += 3\n",
    "outlier_data.species.replace([1, 2, 3, 4, 5, 6], ['Setosa', 'Versicolor', 'Virginica', 'Outliers Setosa', 'Outliers Versicolor', 'Outliers Virginica'], inplace=True)\n",
    "sns.pairplot(outlier_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], height=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Which outlier treatment order would you choose?:\n",
    "- First individual, then colective? => Then the ellipsoid for collective changes after individual.\n",
    "- First colective, then individual? => Then the boxplots change after collective.\n",
    "- Both in parallel? => Then present outliers affect both the ellipsoid and the boxplots.\n",
    "\n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "[your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do it here in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "outlier_data.species.replace(['Setosa', 'Versicolor', 'Virginica', 'Outliers Setosa', 'Outliers Versicolor', 'Outliers Virginica'], [1, 2, 3, 1, 2, 3], inplace=True)\n",
    "outlier_free_data = outlier_data\n",
    "outlier_positions = list(np.sort(outlier_positions_mah + outlier_positions_box))\n",
    "outlier_free_data.drop(outlier_free_data.index[outlier_positions], inplace=True)\n",
    "outlier_free_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Now we should have guessed correctly that the number of rows to be kept would be 131 (i.e., deleting 19 = 15 + 4) because both types of outliers were from different classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_free_data.species.replace([1, 2, 3], ['Setosa', 'Versicolor', 'Virginica'], inplace=True)\n",
    "sns.pairplot(outlier_free_data, hue='species', vars=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], height=2.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now all three classes are almost already separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For applying the Fayyad-Irani MDLP discretization algorithm, there is not an implementation in any standard Python package. Therefore, we will opt for other alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, discretized datasets for both raw and clean data are available locally for loading in csv format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is not a goal by itself. Its aim is to prepare the data for a posterior task (learning). The task we will choose is a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "print(\"Some data\")\n",
    "display(Image(filename='DecisionTreeSampleData.png'))\n",
    "\n",
    "print(\"Decision tree levels\")\n",
    "display(Image(filename='DecisionTreeLevels.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first level (left picture) we see the cutpoint in the value $y = 6$, dividing the space into two hemispheres. Notice that in the upper half all points are from the same class, thus this part of the space will not be further divided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second level (middle picture), once divided the space in the previous step and fixed the upper hemisphere , we devide the lower one into two hemispheres using the cutpoint $x = -0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third level (right picture), both parts will be split into two because the previous division could not isolate points from one unique class. The new cutpoints are now $y = 4$ (left part) and $y = 2.5$ (right part). The process continues until complete class isolation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the performance of our eight Iris datasets:\n",
    "- Raw data (provided file iris_data.csv),\n",
    "- Raw MDLP discretized data (provided file iris_MDLP_data.csv),\n",
    "- Raw equal-width discretized data,\n",
    "- Raw equal-frequency discretized data,\n",
    "- Cleaned data (provided file iris_clean_data.csv),\n",
    "- Cleaned MDLP discretized data (provided file iris_MDLP_clean_data.csv),\n",
    "- Cleaned equal-width discretized data, and\n",
    "- Cleaned equal-frequency discretized data.\n",
    "\n",
    "We will use decision tree as model and F1 score as validation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function automatic_score computes the average F1 score of the 5 steps in 5-Fold CV, using decision tree as estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def automatic_scoring(df):\n",
    "    algorithm = DecisionTreeClassifier()\n",
    "    score = cross_val_score(estimator=algorithm, X=df.values[:, :-1], y=df.values[:, -1], cv=5, scoring='f1_macro')\n",
    "    summary_score = score.mean()\n",
    "    return summary_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we calculate the score for the cleaned data case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.read_csv('iris_clean_data.csv')\n",
    "clean_data_score = automatic_scoring(clean_data)\n",
    "clean_data_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1:\n",
    "Obtain the 4 not provided datasets. Obtain the score for the 8 datasets using automatic_scoring function. Which preprocessing scheme performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Remember that the Scikit-Learn function needed is KBinsDiscretizer. Execute next cell for further info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "?KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: Take into account the difference between a Pandas dataframe and a Numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# In general, and for your future revisions of the material, it is better that you provided a complete code here.\n",
    "# So it is better to define imports and functions here, so that this one single cell could be executed on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore principal component analysis (PCA). We will see how many principal components (PCs) are selected when we want to capture at least 99% of the total variance, as well as the linear combinations defining them. We do not need to perform mean centering because PCA from Scikit-Learn does it internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First with raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('iris_data.csv')\n",
    "raw_data.species.replace(['Setosa', 'Versicolor', 'Virginica'], [1, 2, 3], inplace=True)\n",
    "X_raw = raw_data[raw_data.columns[:-1]]\n",
    "pca.fit(X_raw)\n",
    "X_reduced_raw = pca.transform(X_raw)\n",
    "raw_pca_data = pd.DataFrame(data=X_reduced_raw, columns=['PC1', 'PC2', 'PC3'])\n",
    "# NOTE: The names of the columns were given knowing that the amount of selected PCs was going to be 3\n",
    "# In the next piece of code we generalize it using a list comprehension\n",
    "raw_pca_data = pd.concat([raw_pca_data, raw_data[raw_data.columns[-1]]], axis=1)\n",
    "print(\"There have been selected \" + str(X_reduced_raw.shape[1]) + \" principal components.\")\n",
    "print(\"Meaning of the \" + str(X_reduced_raw.shape[1]) + \" components:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name) for value, name in zip(component, [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.read_csv('iris_clean_data.csv')\n",
    "clean_data.species.replace(['Setosa', 'Versicolor', 'Virginica'], [1, 2, 3], inplace=True)\n",
    "X_clean = clean_data[clean_data.columns[:-1]]\n",
    "pca.fit(X_clean)\n",
    "X_reduced_clean = pca.transform(X_clean)\n",
    "clean_pca_data = pd.DataFrame(data=X_reduced_clean, columns=['PC' + str(x) for x in list(range(1, X_reduced_clean.shape[1] + 1))])\n",
    "clean_pca_data = pd.concat([clean_pca_data, clean_data[clean_data.columns[-1]]], axis=1)\n",
    "print(\"There have been selected \" + str(X_reduced_clean.shape[1]) + \" principal components.\")\n",
    "print(\"Meaning of the 2 components:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name) for value, name in zip(component, [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the first two PCs by means of scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NOTE: This 2-D plot with the first two PCs would rise an error if the amount of selected PCs were 1 (e.g. if PCA(n_components=0.90))\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(16, 6), ncols=2)\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.3)\n",
    "y_raw = raw_data.species.values\n",
    "y_clean = clean_data.species.values\n",
    "ax1.scatter(X_reduced_raw[:, 0], X_reduced_raw[:, 1], c=y_raw, alpha = 1.0)\n",
    "ax1.set_title('Raw data')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax2.scatter(X_reduced_clean[:, 0], X_reduced_clean[:, 1], c=y_clean, alpha = 1.0)\n",
    "ax2.set_title('Clean data')\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expect better performance with the clean data, at least for low depth levels. We will check it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: \n",
    "Get the proyected data (both raw and clean) capturing 95% of the variance, and calculate the performance of both using the same modeling scheme used before (5-Fold CV with desision trees by meand of F1 score).\n",
    "\n",
    "Is the performance better in the case of clean data?\n",
    "\n",
    "Is the performance better with PCA projections (so using less than 4 extracted variables) than with the initial data (4 original variables)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# In general, and for your future revisions of the material, it is better that you provided a complete code here.\n",
    "# So it is better to define imports and functions here, so that this one single cell could be executed on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with feature selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have 4 features, there are only 6 posible subsets of size 2. We consider all 6 2-features subsets, focusing only in the clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** [Note: to be answered without any code (just feelings. There are not right or wrong answers)] \n",
    "- Which pair do you think will work the best? Better than PCs? Better than the original 4-features data? Why?\n",
    "- Which do you think it is the (theoretical) advantage of using 2PCs instead of 2 original variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: every answer we provide using a model (decision trees here) is valid in that environment. A different model could provide a different answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "\n",
    "[Your answers here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: \n",
    "Focusing only in clean data, obtain a vector containing the 6 score values for the 6 possible 2-features subsets.\n",
    "For the selected modeling scheme, which preprocessing scheme seemed to be the best? \n",
    "\n",
    "Note: Try to code the first part of the solution in an efficient way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# In general, and for your future revisions of the material, it is better that you provided a complete code here.\n",
    "# So it is better to define imports and functions here, so that this one single cell could be executed on its own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control point: Modeling and validation\n",
    "#### Máster en Análisis de Datos, Ciberseguridad y Computación en la Nube\n",
    "#### Aprendizaje Automático - Punto de Control 2 (11/11/2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This control point has two parts, one corresponding to a classification problem and the other one to a regression problem. You receive both parts at the same time. They do not necessarily take the same time for solving.\n",
    "\n",
    "The main objective in each part of the control point is to solve a complete problem, including a soft preprocessing (necessary functions are provided below), model selection and model validation. \n",
    "\n",
    "You must take into account that the datasets are not toy datasets, and their sizes could be relevant.\n",
    "\n",
    "Below you have auxiliary functions for preprocessing the data. They offer margin for slight adaptations, which could provide an extra pump to the scores you will get.\n",
    "\n",
    "If you try anytime several options, it is important to show the results of those discarded trials, because what is not visible cannot be evaluated.\n",
    "\n",
    "The deliverable of this control point is this Jupyter Notebook containing the code, plus some short answers in markdown cells if required. Comments inside the code are meant for helping understanding the code, not fot providing answers to the questions. Use markdown cells for that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Keep in mind that some functions accept both Pandas dataframes and Numpy arrays, but some others only one of them. Nevertheless, we should know how to pass form one to the other and vice versa.\n",
    "\n",
    "NOTE: Keep in mind that some functions will take some time to run. You can continue working on other cells during the run to avoid wasting time waiting.\n",
    "\n",
    "REMINDER: Do not use python packages that we have not seen in the lecture.\n",
    "\n",
    "REMINDER: The test must be written in ENGLISH. I will not consider any text or comment inside code if it is not written in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliar functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **feature_selection_model** fits a SelectPercentile model on input (x) and target (y) using the selected score (default=chi2) and percentile (default=5).\n",
    "\n",
    "The options for scores are: mutual_info_classif, mutual_info_regression, f_classif, f_regression and chi2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif, chi2\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression\n",
    "\n",
    "def feature_selection_model(x, y, score=chi2, percentile=5):    \n",
    "    fsm = SelectPercentile(score, percentile=percentile).fit(x, y)\n",
    "    return fsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **imbalance_correction_model_fitting** provides a way for applying undersampling, oversampling or mixed (oversampling + undersampling) strategies on some data, where we denote inputs as *x* and target as *y*, as well as a *model* that will be fit right after the correction on the balanced data. The parameter *strategy* (accepting values 'over', 'under' and 'mixed') determines the chosen strategy.\n",
    "\n",
    "The options for undersampling are: 'EditedNearestNeighbours' and 'TomekLinks'. BorderlineSMOTE (version 1) is the only accepted overfitting technique, and will be used despite of the parameter you use in *over*.\n",
    "\n",
    "The output is the model fitted for the provided data.\n",
    "\n",
    "These days, the imbalanced-learn website was unavailable. In case you need to check the documentation, it is still available in the project github page (https://github.com/scikit-learn-contrib/imbalanced-learn/tree/master/doc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "def imbalance_correction_model_fitting(x, y, under, strategy, model, over='BorderlineSMOTE'): \n",
    "    over = BorderlineSMOTE(kind='borderline-1', random_state=seed)\n",
    "    if under is 'TomekLinks':\n",
    "        under = TomekLinks(random_state=seed)\n",
    "    else:\n",
    "        under = EditedNearestNeighbours(random_state=seed)\n",
    "    if strategy is 'over':\n",
    "        scheme = make_pipeline(over, model)\n",
    "    elif strategy is 'under':\n",
    "        scheme = make_pipeline(under, model)\n",
    "    else:\n",
    "        scheme = make_pipeline(over, under, model)\n",
    "    \n",
    "    fitted_model = scheme.fit(x, y)\n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset 'sekholopane.csv' has 100 variables and almost 2000 samples. We will assume that it has neither outliers nor missing values. You have to perform the following tasks:\n",
    "* (0) [0.25 points] Fix a seed for all random_state parameter. Use the sum of the digits of your DNI. Use this seed in all functions that have random_state parameter. \n",
    "<br><br>\n",
    "* (1) [0.25 points] Split the data into train and test sets, keeping two thirds of the data for training. Make sure you keep the ratio of all the classes in both parts.\n",
    "<br><br>\n",
    "* (2) [1.5 points] Tune one of the SVC algorithms. Optimize the two more relevant parameters of the algorithm you have chosen, by means of 10-fold cross validation. Make sure that you check at least 17 different parameters combinations.\n",
    "<br><br>\n",
    "* (3) [1 points] Tune a neural network, optimizing the layers topology with at least 5 different topologies. Keep the rest of parameters with their default values, except random_state (where you must use your seed) and max_iter (use 1000).\n",
    "<br><br>\n",
    "* (4) [1 points] Tune a random forest algorithm, optimizing n_estimators and max_depth with at least 4 different values for both parameters. Comparing (4) with (2) and (3), which is the overall winning model? (Note: by winning model we mean the best paradigm + parameters. One example could be random forest with n_estimators=3 and max_depth=700).\n",
    "<br><br>\n",
    "* (5) [0.5 points] Would you say that the data is imbalanced? Looking at the available options for feature selection and for imbalance correction, how many combined (feature selection + imbalance correction) preprocessing schemes can you define?\n",
    "<br><br>\n",
    "* (6) [1 points] For the overall winning model, check whether performing any of the possible preprocessing schemes performs even better. \n",
    "<br><br>\n",
    "* (7) [0.5 points] For the overall best model you have found in (6), obtain the classification report, the area under the ROC curve, and the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.25 points] Fix a seed for all random_state parameter. Use the sum of the digits of your DNI. Use this seed in all functions that have random_state parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72841579\n",
    "seed = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.25 points] Split the data into train and test sets, keeping two thirds of the data for training. Make sure you keep the ratio of all the classes in both parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "      <th>v100</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1    v2    v3    v4    v5    v6    v7    v8    v9   v10  ...   v92  \\\n",
       "0  0.19  0.33  0.02  0.90  0.12  0.17  0.34  0.47  0.29  0.32  ...  0.12   \n",
       "1  0.00  0.16  0.12  0.74  0.45  0.07  0.26  0.59  0.35  0.27  ...  0.21   \n",
       "2  0.00  0.42  0.49  0.56  0.17  0.04  0.39  0.47  0.28  0.32  ...  0.14   \n",
       "3  0.04  0.77  1.00  0.08  0.12  0.10  0.51  0.50  0.34  0.21  ...  0.19   \n",
       "4  0.01  0.55  0.02  0.95  0.09  0.05  0.38  0.38  0.23  0.36  ...  0.11   \n",
       "\n",
       "    v93   v94   v95   v96   v97   v98   v99  v100  class  \n",
       "0  0.42  0.50  0.51  0.64  0.12  0.26  0.20  0.32   -1.0  \n",
       "1  0.50  0.34  0.60  0.52  0.02  0.12  0.45  0.00    1.0  \n",
       "2  0.49  0.54  0.67  0.56  0.01  0.21  0.02  0.00   -1.0  \n",
       "3  0.30  0.73  0.64  0.65  0.02  0.39  0.28  0.00   -1.0  \n",
       "4  0.72  0.64  0.61  0.53  0.04  0.09  0.02  0.00   -1.0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('sekholopane.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.5 points] Tune one of the SVC algorithms. Optimize the two more relevant parameters of the algorithm you have chosen, by means of 10-fold cross validation. Make sure that you check at least 17 different parameters combinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'gamma': 0.1, 'kernel': 'poly', 'random_state': 43}\n",
      "roc_auc_score: 0.623431855500821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "tuned_parameters = {\n",
    "        \"random_state\": [seed],\n",
    "        \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "        \"gamma\": ['auto','scale', 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"best params: \" + str(clf.best_params_))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"roc_auc_score: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 points] Tune a neural network, optimizing the layers topology with at least 5 different topologies. Keep the rest of parameters with their default values, except random_state (where you must use your seed) and max_iter (use 1000). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'hidden_layer_sizes': 35, 'random_state': 43}\n",
      "roc_auc_score: 0.7201477832512315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "tuned_parameters = {\n",
    "        \"hidden_layer_sizes\": [20,35,50,75,100,],        \n",
    "        \"random_state\": [seed],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(MLPClassifier(), tuned_parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"best params: \" + str(clf.best_params_))\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "print(\"roc_auc_score: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 points] Tune a random forest algorithm, optimizing n_estimators and max_depth with at least 4 different values for both parameters. Comparing (4) with (2) and (3), which is the overall winning model? (Note: by winning model we mean the best paradigm + parameters. One example could be random forest with n_estimators=3 and max_depth=700). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'max_depth': 100, 'n_estimators': 100, 'random_state': 43}\n",
      "roc_auc_score: 0.6450738916256158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "tuned_parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 150],\n",
    "    \"max_depth\": [1, 10, 100, 1000],\n",
    "    \"random_state\": [seed],    \n",
    "}\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"best params: \" + str(clf.best_params_))\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "print(\"roc_auc_score: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the best score (0.72) with Neural Network. With the following params:\n",
    "\n",
    "MLPClassifier(hidden_layer_sizes=35, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.5 points] Would you say that the data is imbalanced? Looking at the available options for feature selection and for imbalance correction, how many combined (feature selection + imbalance correction) preprocessing schemes can you define? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    1844\n",
       " 1.0     150\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, data is inbalanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont have any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FEATURE SELECTION (SELECT PERCENTILE)\n",
    "para ir avanzando mas rapido, he decidido dejar sin hacer esta parte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 points] For the overall winning model, check whether performing any of the possible preprocessing schemes performs even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def score_with_best_model(X_train, X_test, y_train, y_test):\n",
    "    model = MLPClassifier(hidden_layer_sizes=35, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(str(roc_auc_score(y_test, y_pred)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try NearMiss undersampling technique to try to balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to select most relevant features using all the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994, 75)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we maintain 20% of features\n",
    "X_new = pd.DataFrame(feature_selection_model(X, y, score=mutual_info_classif, percentile=20).transform(X))\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_new, y, test_size=0.33, random_state=seed, stratify=y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6817898193760262\n"
     ]
    }
   ],
   "source": [
    "score_with_best_model(X_train1, X_test1, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying with different percentiles we dont get better score so we are going to continue with all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.5 points] For the overall best model you have found in (6), obtain the classification report, the area under the ROC curve, and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=35, random_state=seed)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.96      0.98      0.97       609\n",
      "         1.0       0.66      0.46      0.54        50\n",
      "\n",
      "    accuracy                           0.94       659\n",
      "   macro avg       0.81      0.72      0.75       659\n",
      "weighted avg       0.93      0.94      0.94       659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.7201477832512315\n"
     ]
    }
   ],
   "source": [
    "print('roc_auc_score: '  + str(roc_auc_score(y_test, y_pred)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[597  12]\n",
      " [ 27  23]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset 'superconducting.csv' has 81 variables and more than 21K samples. It is a real-world dataset about predicting the critical temperature of semiconducting materials.\n",
    "\n",
    "* (1) [0.25 points] Split the data into train, validation and test sets, keeping 20% for both validation and test.\n",
    "<br><br>\n",
    "* (2) [1 points] Using the validation data and the mean squared error score in principal components regression (PCR), find the adequate number, $a$, of principal components, up to a maximum of 40. Focusing now on the cummulative variance, is the cummulative variance you are not capturing (because of not selecting the last $81 - a$ PCs) higher than 1 per 1000?\n",
    "<br><br>\n",
    "* (3) [1 points] Now we will check the performance using feature selection, instead of the feature extraction by PCA done in (2). Try all suitable scoring options of the feature_selection_model function, fixing percentile=5. Use multiple linear regression as predictive model. Which compression has been higher?\n",
    "<br><br>\n",
    "* (4) [1 points] Repeat (2) and (3) using ridge regression instead of multiple linear regression. Set the best of all models seen so far, as your winning model.\n",
    "<br><br>\n",
    "* (5) [0.75 points] Check the performance of the winning model on the test set. Compare it with the performance on the original dataset (without preprocessing) using the same predictor as in the winning model (multiple linear regression or ridge regression). Was our preprocessing beneficial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.116612</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.219783</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.066221</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.437059</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>36.396602</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>47.094633</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1.888175</td>\n",
       "      <td>2.210679</td>\n",
       "      <td>1.557113</td>\n",
       "      <td>1.047221</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.468606</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.122509</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.232679</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.029175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.444697</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.119560</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.226222</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.048834</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.440952</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.110716</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.206963</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.096052</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.428809</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "0                   4         88.944468             57.862692   \n",
       "1                   5         92.729214             58.518416   \n",
       "2                   4         88.944468             57.885242   \n",
       "3                   4         88.944468             57.873967   \n",
       "4                   4         88.944468             57.840143   \n",
       "\n",
       "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "0          66.361592              36.116612             1.181795   \n",
       "1          73.132787              36.396602             1.449309   \n",
       "2          66.361592              36.122509             1.181795   \n",
       "3          66.361592              36.119560             1.181795   \n",
       "4          66.361592              36.110716             1.181795   \n",
       "\n",
       "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "0                 1.062396          122.90607              31.794921   \n",
       "1                 1.057755          122.90607              36.161939   \n",
       "2                 0.975980          122.90607              35.741099   \n",
       "3                 1.022291          122.90607              33.768010   \n",
       "4                 1.129224          122.90607              27.848743   \n",
       "\n",
       "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
       "0        51.968828  ...          2.257143       2.213364           2.219783   \n",
       "1        47.094633  ...          2.257143       1.888175           2.210679   \n",
       "2        51.968828  ...          2.271429       2.213364           2.232679   \n",
       "3        51.968828  ...          2.264286       2.213364           2.226222   \n",
       "4        51.968828  ...          2.242857       2.213364           2.206963   \n",
       "\n",
       "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
       "0         1.368922             1.066221              1           1.085714   \n",
       "1         1.557113             1.047221              2           1.128571   \n",
       "2         1.368922             1.029175              1           1.114286   \n",
       "3         1.368922             1.048834              1           1.100000   \n",
       "4         1.368922             1.096052              1           1.057143   \n",
       "\n",
       "   std_Valence  wtd_std_Valence  critical_temp  \n",
       "0     0.433013         0.437059           29.0  \n",
       "1     0.632456         0.468606           26.0  \n",
       "2     0.433013         0.444697           19.0  \n",
       "3     0.433013         0.440952           22.0  \n",
       "4     0.433013         0.428809           23.0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seed = 43\n",
    "\n",
    "df = pd.read_csv('superconducting.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.25 points] Split the data into train, validation and test sets, keeping 20% for both validation and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 points] Using the validation data and the mean squared error score in principal components regression (PCR), find the adequate number, 𝑎, of principal components, up to a maximum of 40. Focusing now on the cummulative variance, is the cummulative variance you are not capturing (because of not selecting the last 81−𝑎 PCs) higher than 1 per 1000? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCA0</th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>PCA3</th>\n",
       "      <th>PCA4</th>\n",
       "      <th>PCA5</th>\n",
       "      <th>PCA6</th>\n",
       "      <th>PCA7</th>\n",
       "      <th>PCA8</th>\n",
       "      <th>PCA9</th>\n",
       "      <th>...</th>\n",
       "      <th>PCA11</th>\n",
       "      <th>PCA12</th>\n",
       "      <th>PCA13</th>\n",
       "      <th>PCA14</th>\n",
       "      <th>PCA15</th>\n",
       "      <th>PCA16</th>\n",
       "      <th>PCA17</th>\n",
       "      <th>PCA18</th>\n",
       "      <th>PCA19</th>\n",
       "      <th>PCA20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.312360</td>\n",
       "      <td>-1.170030</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.153494</td>\n",
       "      <td>0.362203</td>\n",
       "      <td>-0.162382</td>\n",
       "      <td>0.261143</td>\n",
       "      <td>-0.206815</td>\n",
       "      <td>0.361993</td>\n",
       "      <td>0.052164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159684</td>\n",
       "      <td>-0.313973</td>\n",
       "      <td>-0.063068</td>\n",
       "      <td>-0.184779</td>\n",
       "      <td>-0.114812</td>\n",
       "      <td>0.035941</td>\n",
       "      <td>-0.060049</td>\n",
       "      <td>0.126594</td>\n",
       "      <td>0.059511</td>\n",
       "      <td>0.028170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.855798</td>\n",
       "      <td>0.353396</td>\n",
       "      <td>-0.316044</td>\n",
       "      <td>1.671412</td>\n",
       "      <td>-0.025749</td>\n",
       "      <td>0.793829</td>\n",
       "      <td>0.712299</td>\n",
       "      <td>0.586164</td>\n",
       "      <td>-0.530098</td>\n",
       "      <td>0.576396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003891</td>\n",
       "      <td>-0.348147</td>\n",
       "      <td>0.041137</td>\n",
       "      <td>0.306338</td>\n",
       "      <td>0.205001</td>\n",
       "      <td>0.239133</td>\n",
       "      <td>0.164228</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>0.102803</td>\n",
       "      <td>0.184826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.448908</td>\n",
       "      <td>0.339928</td>\n",
       "      <td>-0.855757</td>\n",
       "      <td>1.965854</td>\n",
       "      <td>-0.291555</td>\n",
       "      <td>0.462020</td>\n",
       "      <td>0.813965</td>\n",
       "      <td>0.330929</td>\n",
       "      <td>-0.495265</td>\n",
       "      <td>0.319464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003597</td>\n",
       "      <td>-0.021071</td>\n",
       "      <td>-0.290377</td>\n",
       "      <td>0.259515</td>\n",
       "      <td>0.396133</td>\n",
       "      <td>-0.100374</td>\n",
       "      <td>0.057554</td>\n",
       "      <td>0.099562</td>\n",
       "      <td>0.098758</td>\n",
       "      <td>-0.200112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.247560</td>\n",
       "      <td>-0.356068</td>\n",
       "      <td>0.464626</td>\n",
       "      <td>0.712587</td>\n",
       "      <td>0.072139</td>\n",
       "      <td>-0.259296</td>\n",
       "      <td>-0.068752</td>\n",
       "      <td>-0.282230</td>\n",
       "      <td>0.199696</td>\n",
       "      <td>-0.020204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008447</td>\n",
       "      <td>-0.205174</td>\n",
       "      <td>0.255761</td>\n",
       "      <td>0.046238</td>\n",
       "      <td>-0.078202</td>\n",
       "      <td>-0.096612</td>\n",
       "      <td>-0.066516</td>\n",
       "      <td>-0.162972</td>\n",
       "      <td>-0.057491</td>\n",
       "      <td>0.135162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.537044</td>\n",
       "      <td>-0.329914</td>\n",
       "      <td>-0.586943</td>\n",
       "      <td>0.251302</td>\n",
       "      <td>-0.326870</td>\n",
       "      <td>-0.432467</td>\n",
       "      <td>-0.005168</td>\n",
       "      <td>-0.639395</td>\n",
       "      <td>-0.122173</td>\n",
       "      <td>-0.202462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.278517</td>\n",
       "      <td>-0.193945</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.306393</td>\n",
       "      <td>0.058638</td>\n",
       "      <td>0.184288</td>\n",
       "      <td>-0.029122</td>\n",
       "      <td>0.126803</td>\n",
       "      <td>-0.016718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PCA0      PCA1      PCA2      PCA3      PCA4      PCA5      PCA6  \\\n",
       "0 -0.312360 -1.170030  0.208600  0.153494  0.362203 -0.162382  0.261143   \n",
       "1  0.855798  0.353396 -0.316044  1.671412 -0.025749  0.793829  0.712299   \n",
       "2 -0.448908  0.339928 -0.855757  1.965854 -0.291555  0.462020  0.813965   \n",
       "3 -0.247560 -0.356068  0.464626  0.712587  0.072139 -0.259296 -0.068752   \n",
       "4 -0.537044 -0.329914 -0.586943  0.251302 -0.326870 -0.432467 -0.005168   \n",
       "\n",
       "       PCA7      PCA8      PCA9  ...     PCA11     PCA12     PCA13     PCA14  \\\n",
       "0 -0.206815  0.361993  0.052164  ...  0.159684 -0.313973 -0.063068 -0.184779   \n",
       "1  0.586164 -0.530098  0.576396  ... -0.003891 -0.348147  0.041137  0.306338   \n",
       "2  0.330929 -0.495265  0.319464  ... -0.003597 -0.021071 -0.290377  0.259515   \n",
       "3 -0.282230  0.199696 -0.020204  ...  0.008447 -0.205174  0.255761  0.046238   \n",
       "4 -0.639395 -0.122173 -0.202462  ...  0.003045  0.278517 -0.193945  0.220143   \n",
       "\n",
       "      PCA15     PCA16     PCA17     PCA18     PCA19     PCA20  \n",
       "0 -0.114812  0.035941 -0.060049  0.126594  0.059511  0.028170  \n",
       "1  0.205001  0.239133  0.164228  0.188800  0.102803  0.184826  \n",
       "2  0.396133 -0.100374  0.057554  0.099562  0.098758 -0.200112  \n",
       "3 -0.078202 -0.096612 -0.066516 -0.162972 -0.057491  0.135162  \n",
       "4  0.306393  0.058638  0.184288 -0.029122  0.126803 -0.016718  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Calculates dataframe PCA\n",
    "def get_pca(X, pca): \n",
    "    pca = PCA(n_components=pca)\n",
    "    pca.fit(X)\n",
    "    X_reduced = pca.transform(X)\n",
    "    # print(\"There have been selected \" + str(X_reduced.shape[1]) + \" principal components.\")    \n",
    "    columns = []\n",
    "    for n in range(X_reduced.shape[1]):\n",
    "        columns.append(\"PCA\" + str(n))    \n",
    "    df = pd.DataFrame(X_reduced, columns=columns)\n",
    "    return df  \n",
    "\n",
    "X_train_pca = get_pca(X_train, 0.9)\n",
    "\n",
    "X_train_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 0.90, we get 21 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error (PCA): 0.23120946360009703\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression().fit(X_train_pca, y_train)\n",
    "y_pred = model.predict(get_pca(X_val, components))\n",
    "error = mean_squared_error(y_pred, y_val)\n",
    "\n",
    "print(\"mean_squared_error (PCA): \" + str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 points] Now we will check the performance using feature selection, instead of the feature extraction by PCA done in (2). Try all suitable scoring options of the feature_selection_model function, fixing percentile=5. Use multiple linear regression as predictive model. Which compression has been higher? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 20)\n",
      "(319, 20)\n"
     ]
    }
   ],
   "source": [
    "model = feature_selection_model(X_train, y_train, percentile=percentile)\n",
    "X_train_selec = model.transform(X_train)\n",
    "X_val_selec = model.transform(X_val)\n",
    "print(X_train_selec.shape)\n",
    "print(X_val_selec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced to 15 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error (chi2): 0.16510738819304246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression().fit(X_train_selec, y_train)\n",
    "y_pred = model.predict(X_val_selec)\n",
    "error = mean_squared_error(y_pred, y_val)\n",
    "print(\"mean_squared_error (chi2): \" + str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that the best score (error) we can get is 0, we get better score selection %20 of features with chi2 than  using 21 PCAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 points] Repeat (2) and (3) using ridge regression instead of multiple linear regression. Set the best of all models seen so far, as your winning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (319,21) and (20,) not aligned: 21 (dim 1) != 20 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-520b3ac17065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpca_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# tiene la misma cantidad de features que en X_train_pca\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                                                     \u001b[1;31m# no se porque me da error de dimensiones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\AI\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \"\"\"\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\AI\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m--> 206\u001b[1;33m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (319,21) and (20,) not aligned: 21 (dim 1) != 20 (dim 0)"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pca_model = Ridge(alpha=1.0).fit(X_train_pca, y_train)\n",
    "y_pred = model.predict(get_pca(X_val, components)) # tiene la misma cantidad de features que en X_train_pca\n",
    "                                                    # no se porque me da error de dimensiones\n",
    "error = mean_squared_error(y_pred, y_val)\n",
    "\n",
    "print(\"mean_squared_error(PCA+Ridge): \" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error (chi2+Ridge): 0.16510738819304246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "feature_selection_model = Ridge(alpha=1.0).fit(X_train_selec, y_train)\n",
    "y_pred = model.predict(X_val_selec)\n",
    "error = mean_squared_error(y_pred, y_val)\n",
    "print(\"mean_squared_error (chi2+Ridge): \" + str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.75 points] Check the performance of the winning model on the test set. Compare it with the performance on the original dataset (without preprocessing) using the same predictor as in the winning model (multiple linear regression or ridge regression). Was our preprocessing beneficial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como me ha dado error al usar PCA + Ridge, voy a asumir para el mejor son los datos seleccionados con Chi2, usandolo en este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error (original data): 0.1911301686258577\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_val = X_train.copy()\n",
    "X_train_val = X_train_val.append(X_val)\n",
    "\n",
    "y_train_val = y_train.copy()\n",
    "y_train_val = y_train_val.append(y_val)\n",
    "\n",
    "model = Ridge(alpha=1.0).fit(X_train_val, y_train_val)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "error = mean_squared_error(y_pred, y_test)\n",
    "print(\"mean_squared_error (original data): \" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

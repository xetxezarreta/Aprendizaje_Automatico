{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing clustering, categorical features become problematic, because the usual distance for categorical features gives bad results using, for instance K-Means.\n",
    "\n",
    "That distance is given by\n",
    "$$d(a, b) = \\left\\{\n",
    "\\begin{array}{rcl}\n",
    "     1 & \\mbox{ , if } & a = b\n",
    "  \\\\ 0 & \\mbox{ , if } & a\\neq b\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "In order to adapt K-Means to the case of categorical features, there is an algorithm called K-Modes that is able to handle such features in an efficient way.\n",
    "\n",
    "The documentation of kmodes package is available here https://pypi.org/project/kmodes/. In order to install it, you will have to use pip because it is not available in any Anaconda package repository. In general, it is not recommended, for Anaconda users, to install packages using pip. In this case, I had never have problems with this package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset 'USCrimeMDLP.csv' contains 300 samples and 79 categorical features, plus a class feature. Use that dataset for the following exercises:\n",
    "1.\tSplit the data into train and test, keeping 200 samples for training. Use stratification. Always use random_state=0.\n",
    "<br><br>\n",
    "2.\tUse K-Means algorithm with 2clusters, and evaluate it using the area under the ROC curve (AUC) as external measure (we can do it because the class column is available). Take into account that the classes are {-1, 1} and the clusters names are {0, 1}. Did you get a strange value? Why can AUC be so low? Correct the problem. (Hint: the prediction made by clustering assigns the name of the cluster as the class, but the names are just tag names). Is the K-Means algorithm adequate in this case?\n",
    "<br><br>\n",
    "3.\tConsider the clustering you have obtained as a classification algorithm, i.e. each cluster predicting a class. Apart from the AUC, obtained above, calculate the classification report, area under the ROC curve (AUC), and confusion matrix. \n",
    "<br><br>\n",
    "4.\tCompare these results with the classification performed with random forest (n_estimators=100), and with SVC (C=2.0) and NuSVC (nu=0.001).\n",
    "<br><br>\n",
    "5.\tUse K-Modes algorithm with 2 clusters, evaluating it in the same way as with K-Means. Comparing with the result in exercise 2, should we discard kmodes as an unsupervised classification procedure?\n",
    "<br><br>\n",
    "6.\tMake summary comments of all the exercises as a general conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 100, -1: 100})\n",
      "Counter({1: 50, -1: 50})\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('USCrimeMDLP.csv')\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(df.values[:,:-1], df.values[:,-1], test_size=100, random_state=0, stratify=df.values[:,-1])\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that data is stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score test: 0.31000000000000005\n"
     ]
    }
   ],
   "source": [
    "# 2.\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(2, random_state=0)  # 2 clusters\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "kmeans.fit(X_train)\n",
    "y_pred_means = kmeans.predict(X_test)\n",
    "\n",
    "AUC = roc_auc_score(y_test, y_pred_means)\n",
    "print(\"score test: \" + str(AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtainer AUC score isvery low. Thats is due to the fact that data is being classified using 0 and 1 values rather tahn -1 and 1. This happens because kmeans doesnt know the target class names.\n",
    "\n",
    "Not only that, the low score also could mean that is predicting:\n",
    "0 when it belongs to -1\n",
    "1 when it belongs to 1\n",
    "\n",
    "And in reality it is the oposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_means = y_pred_means * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score test: 0.69\n"
     ]
    }
   ],
   "source": [
    "AUC = roc_auc_score(y_test, y_pred_means)\n",
    "print(\"score test: \" + str(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.42      0.58        50\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.21       100\n",
      "   macro avg       0.30      0.14      0.19       100\n",
      "weighted avg       0.46      0.21      0.29       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haritz\\Miniconda3\\envs\\faa\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Haritz\\Miniconda3\\envs\\faa\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 3.\n",
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_means))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix test: \n",
      "\n",
      "[[21 29  0]\n",
      " [ 0  0  0]\n",
      " [ 2 48  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"confusion matrix test: \\n\")\n",
    "print(confusion_matrix(y_test, y_pred_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ROC AUC score test: 0.69\n"
     ]
    }
   ],
   "source": [
    "AUC = roc_auc_score(y_test, y_pred_means)\n",
    "print(\"\\n ROC AUC score test: \" + str(AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the confusion matrix the following results:\n",
    "\n",
    "TP = 21\n",
    "\n",
    "FP = 2\n",
    "\n",
    "TN = 48\n",
    "\n",
    "FN = 29\n",
    "\n",
    "We see that we have a really high FN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF score: 0.86\n",
      "SVC score: 0.84\n",
      "NuSVC score: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haritz\\Miniconda3\\envs\\faa\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Haritz\\Miniconda3\\envs\\faa\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 4.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import NuSVC\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "svc  = SVC(C=2.0, random_state=0)\n",
    "nu_svc = NuSVC(nu=0.001, random_state=0)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "nu_svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "y_pred_nu_svc = nu_svc.predict(X_test)\n",
    "\n",
    "print(\"RF score: \" + str(roc_auc_score(y_test, y_pred_rf)))\n",
    "print(\"SVC score: \" + str(roc_auc_score(y_test, y_pred_svc)))\n",
    "print(\"NuSVC score: \" + str(roc_auc_score(y_test, y_pred_nu_svc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithms perform better compared to KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score test: 0.72\n"
     ]
    }
   ],
   "source": [
    "#5.\n",
    "from kmodes.kmodes import KModes\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "km = KModes(n_clusters=2, random_state=0)\n",
    "\n",
    "km.fit(X_train)\n",
    "\n",
    "y_pred_kmodes = km.predict(X_test)\n",
    "\n",
    "y_pred_kmodes = y_pred_kmodes * -1\n",
    "\n",
    "AUC = roc_auc_score(y_test, y_pred_kmodes)\n",
    "print(\"score test: \" + str(AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.\n",
    "[your conclusion here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "Let's recap all the results.\n",
    "\n",
    "KMeans = 0.69\n",
    "\n",
    "KModes = 0.72\n",
    "\n",
    "NuSVC = 0.88\n",
    "\n",
    "SVC = 0.84\n",
    "\n",
    "RandomForest = 0.86\n",
    "\n",
    "We can observe that the clustering methods have a lower score than the 3 proposed witch happen to be supervised. \n",
    "\n",
    "When comparing the clustering methods, we observe that the KModes has a better score than the other one. And that makes sense dude to the fact that KMeans is not the best option when predicting using categorical data. Rather we should choose KModes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

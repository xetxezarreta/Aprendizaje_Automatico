{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING (Part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a powerful package written in Python and developed by part of the developers of Scikit-Learn, called Imbalanced-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is developed through GitHub (see https://github.com/scikit-learn-contrib/imbalanced-learn), and there is also an official website (see http://imbalanced-learn.org/en/stable/) where you can find all the info you might need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I strongly recommend to read the user guide (see http://imbalanced-learn.org/en/stable/user_guide.html) as well as the general examples as a complement to it (see http://imbalanced-learn.org/en/stable/auto_examples/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package is not available through Anaconda Navigator, but you can install install is from the prompt by entering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try NearMiss undersampling technique on Iris dataset. Since Iris is perfectly balanced, firstly we will imbalance it artificially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training statistics: Counter({1: 38, 2: 38, 0: 17})\n",
      "Testing statistics: Counter({1: 12, 2: 12, 0: 8})\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      1.00      1.00      1.00      1.00      1.00         8\n",
      "          1       1.00      0.83      1.00      0.91      0.91      0.82        12\n",
      "          2       0.86      1.00      0.90      0.92      0.95      0.91        12\n",
      "\n",
      "avg / total       0.95      0.94      0.96      0.94      0.95      0.90        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# Load dataset and create an artificial imbalance\n",
    "iris = load_iris()\n",
    "X, y = make_imbalance(iris.data, iris.target,\n",
    "                      sampling_strategy={0: 25, 1: 50, 2: 50},\n",
    "                      random_state=RANDOM_STATE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)\n",
    "\n",
    "print('Training statistics: {}'.format(Counter(y_train)))\n",
    "print('Testing statistics: {}'.format(Counter(y_test)))\n",
    "\n",
    "# Creation of a pipeline, i.e. concatenation of steps in a composed process (see documentation for further details)\n",
    "pipeline = make_pipeline(NearMiss(version=2),\n",
    "                         LinearSVC(random_state=RANDOM_STATE, max_iter=10000))\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Classification and results presentation\n",
    "print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try now SMOTE oversampling technique on a dataset about thyroid sickness. It has 3772 samples and 52 independent variables. It is imbalanced by a rate of 15 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training statistics: Counter({-1: 2662, 1: 167})\n",
      "Testing statistics: Counter({-1: 879, 1: 64})\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.99      0.99      0.86      0.99      0.92      0.86       879\n",
      "          1       0.90      0.86      0.99      0.88      0.92      0.84        64\n",
      "\n",
      "avg / total       0.98      0.98      0.87      0.98      0.92      0.86       943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load dataset\n",
    "tiroides = pd.read_csv('data/Thyroids.csv')\n",
    "tiroides.values.astype(float)\n",
    "\n",
    "# Separate inputs and target\n",
    "X = tiroides.values[:,:-1]\n",
    "y = tiroides.values[:,-1].astype(int)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)\n",
    "\n",
    "print('Training statistics: {}'.format(Counter(y_train)))\n",
    "print('Testing statistics: {}'.format(Counter(y_test)))\n",
    "\n",
    "# Pipeline creation\n",
    "pipeline = make_pipeline(SMOTE(random_state=RANDOM_STATE),\n",
    "                         RandomForestClassifier(n_estimators=10, random_state=RANDOM_STATE))\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Classification and results presentation\n",
    "print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:\n",
    "\n",
    "(i) Try a different NearMiss version from the one in the example for the thyroids dataset with random forests classifier. Does it get better if we increase the number of trees to 100 in the forest (n_estimators)? And from 100 to 1000?\n",
    "\n",
    "(ii) Plan a mixed strategy for thyroids dataset and chech its performance with random forests. Play with n_estimators parameter to increase f1 average score. Is the order of the mixed sampling strategies relevant?\n",
    "\n",
    "(iii) Combine PCA with the mixed strategy. Quantify the percentage of data compression when capturing 95% of the total cummulative variance. Compare the performance with the one in (ii). In case of big differencies, which could be one reason?\n",
    "\n",
    "(iv) Compare the results of all strategies with the case of not correcting the imbalance.\n",
    "\n",
    "(v) Use ADASYN oversampling technique combined with an undersampling technique different from NearMiss. Explain the reason for your choice. See imbalanced-learn documentation for seeing which functions to use and checking how to use them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# In general, and for your future revisions of the material, it is better that you provided a complete code here.\n",
    "# So it is better to define imports and functions here, so that this one single cell could be executed on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Try a different NearMiss version from the one in the example for the thyroids dataset with random forests classifier. Does it get better if we increase the number of trees to 100 in the forest (n_estimators)? And from 100 to 1000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline with 10 trees and NearMiss version 1\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.99      0.79      0.92      0.88      0.85      0.72       879\n",
      "          1       0.24      0.92      0.79      0.39      0.85      0.74        64\n",
      "\n",
      "avg / total       0.94      0.80      0.91      0.85      0.85      0.72       943\n",
      "\n",
      "Pipeline with 100 trees and NearMiss version 1\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       1.00      0.78      0.97      0.88      0.87      0.75       879\n",
      "          1       0.25      0.97      0.78      0.39      0.87      0.77        64\n",
      "\n",
      "avg / total       0.95      0.80      0.96      0.84      0.87      0.75       943\n",
      "\n",
      "Pipeline with 1000 trees and NearMiss version 1\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       1.00      0.79      0.97      0.88      0.88      0.75       879\n",
      "          1       0.25      0.97      0.79      0.40      0.88      0.78        64\n",
      "\n",
      "avg / total       0.95      0.80      0.96      0.85      0.88      0.75       943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 0\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/Thyroids.csv')\n",
    "df.values.astype(float)\n",
    "\n",
    "# Separate inputs and target\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:,-1].astype(int)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "n_trees = [10, 100, 1000]\n",
    "\n",
    "for n in n_trees:\n",
    "    print(f'Pipeline with {n} trees and NearMiss version 1')\n",
    "    pipeline = make_pipeline(NearMiss(version=1), RandomForestClassifier(n_estimators=n, random_state=seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we increase the n_trees, it does not get better. We have the same f1 score with 10 and 1000 trees.\n",
    "- n_estimators = 10        f1_score = 0.85\n",
    "- n_estimators = 100       f1_score = 0.84\n",
    "- n_estimators = 1000      f1_score = 0.85\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Plan a mixed strategy for thyroids dataset and check its performance with random forests. Play with n_estimators parameter to increase f1 average score. Is the order of the mixed sampling strategies relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator = 10\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.99      0.99      0.92      0.99      0.96      0.92       879\n",
      "          1       0.89      0.92      0.99      0.91      0.96      0.91        64\n",
      "\n",
      "avg / total       0.99      0.99      0.93      0.99      0.96      0.92       943\n",
      "\n",
      "n_estimator = 100\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.99      0.99      0.91      0.99      0.95      0.91       879\n",
      "          1       0.91      0.91      0.99      0.91      0.95      0.89        64\n",
      "\n",
      "avg / total       0.99      0.99      0.91      0.99      0.95      0.91       943\n",
      "\n",
      "n_estimator = 1000\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.99      0.99      0.91      0.99      0.95      0.91       879\n",
      "          1       0.89      0.91      0.99      0.90      0.95      0.89        64\n",
      "\n",
      "avg / total       0.99      0.99      0.91      0.99      0.95      0.91       943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# SMOTE + Tomekâ€™s links\n",
    "# Class to perform over-sampling using SMOTE and cleaning using Tomek links.\n",
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.combine.SMOTETomek.html\n",
    "from imblearn.combine import SMOTETomek \n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/Thyroids.csv')\n",
    "df.values.astype(float)\n",
    "\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:,-1].astype(int)\n",
    "\n",
    "seed = 0\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "n_trees = [10, 100, 1000]\n",
    "\n",
    "for n in n_trees:\n",
    "    print ('n_estimator = ' + str(n))\n",
    "    pipeline = make_pipeline(SMOTETomek(random_state=seed), RandomForestClassifier(n_estimators=n, random_state=seed))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get a similar score with all n_estimators. The score is very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Combine PCA with the mixed strategy. Quantify the percentage of data compression when capturing 95% of the total cummulative variance. Compare the performance with the one in (ii). In case of big differencies, which could be one reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.96      0.89      0.55      0.92      0.70      0.50       879\n",
      "          1       0.26      0.55      0.89      0.35      0.70      0.47        64\n",
      "\n",
      "avg / total       0.92      0.86      0.57      0.88      0.70      0.50       943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate inputs and target\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTETomek \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Calculates dataframe PCA\n",
    "def get_df_pca(df):    \n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    pca.fit(X)\n",
    "    X_reduced = pca.transform(X)\n",
    "    # print(\"There have been selected \" + str(X_reduced.shape[1]) + \" principal components.\")    \n",
    "    columns = []\n",
    "    for n in range(X_reduced.shape[1]):\n",
    "        columns.append(\"PCA\" + str(n))    \n",
    "    df = pd.DataFrame(X_reduced, columns=columns)\n",
    "    df['species'] = y\n",
    "    return df       \n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/Thyroids.csv')\n",
    "df.values.astype(float)\n",
    "\n",
    "df_pca = get_df_pca(df)\n",
    "X = df_pca.values[:,:-1]\n",
    "y = df_pca.values[:,-1].astype(int)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "pipeline = make_pipeline(SMOTETomek(random_state=0), RandomForestClassifier(n_estimators=100, random_state=0))\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get a f1-score of 0.88, not a good score compared without using PCA.\n",
    "- With PCA, we reduce de dimension of the data and with SMOTETomek (SMOTE + Tomek) we modify the data previously compressed with PCA. That could be the reason of getting a lower f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Compare the results of all strategies with the case of not correcting the imbalance.\n",
    "We are going to apply PCA and calculate the score, without balancing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.94      0.99      0.11      0.97      0.33      0.12       879\n",
      "          1       0.54      0.11      0.99      0.18      0.33      0.10        64\n",
      "\n",
      "avg / total       0.91      0.93      0.17      0.91      0.33      0.12       943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We get the score of the model without correcting the imbalance data. (To compare with the used strategies)\n",
    "\n",
    "# Separate inputs and target\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTETomek \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Calculates dataframe PCA\n",
    "def get_df_pca(df):    \n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    pca.fit(X)\n",
    "    X_reduced = pca.transform(X)\n",
    "    # print(\"There have been selected \" + str(X_reduced.shape[1]) + \" principal components.\")    \n",
    "    columns = []\n",
    "    for n in range(X_reduced.shape[1]):\n",
    "        columns.append(\"PCA\" + str(n))    \n",
    "    df = pd.DataFrame(X_reduced, columns=columns)\n",
    "    df['species'] = y\n",
    "    return df       \n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/Thyroids.csv')\n",
    "df.values.astype(float)\n",
    "\n",
    "df_pca = get_df_pca(df)\n",
    "X = df_pca.values[:,:-1]\n",
    "y = df_pca.values[:,-1].astype(int)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "pipeline = make_pipeline(RandomForestClassifier(n_estimators=100, random_state=0))\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the f1-score is very bad. \n",
    "- The f1 score for the majority class (-1) is good, but for the minority class (1) it is bad.\n",
    "- We get a better score if we only balance the data, without applying PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (v) Use ADASYN oversampling technique combined with an undersampling technique different from NearMiss. Explain the reason for your choice. See imbalanced-learn documentation for seeing which functions to use and checking how to use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9875472634432899\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "def automatic_scoring(df):\n",
    "    algorithm = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "    score = cross_val_score(estimator=algorithm, X=df.values[:, :-1], y=df.values[:, -1].astype('int'), cv=5, scoring='f1_macro')\n",
    "    summary_score = score.mean()\n",
    "    return summary_score\n",
    "\n",
    "df = pd.read_csv('data/Thyroids.csv')\n",
    "df.values.astype(float)\n",
    "\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:,-1].astype(int)\n",
    "\n",
    "ada = ADASYN(random_state=0)\n",
    "X_res1, y_res1 = ada.fit_resample(X, y)\n",
    "\n",
    "cnn = TomekLinks()\n",
    "X_res2, y_res2 = cnn.fit_resample(X_res1, y_res1)\n",
    "\n",
    "df = pd.DataFrame(X_res2, columns=df.columns[:-1])\n",
    "df['target'] = y_res2\n",
    "\n",
    "print(automatic_scoring(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TomekLinks undersampling method, we get a high score with cross validation. In this algorithm, we end up removing the majority element from the Tomek link which provides a better decision boundary for a classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

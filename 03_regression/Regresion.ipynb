{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión: Regresión Lineal Simple\n",
    "\n",
    "\n",
    "## References \n",
    "\n",
    "* James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). \n",
    "An introduction to statistical learning (Vol. 112). New York: springer.\n",
    "* Data sets: http://www-bcf.usc.edu/~gareth/ISL/data.html\n",
    "* SCIKIT-LEARN library example http://scikit-learn.org\n",
    "* References Jupyter notebooks:\n",
    "    - R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016)\n",
    "    http://www.science.smith.edu/~jcrouser/SDS293/labs/lab10-py.html\n",
    "    - General Assembly's Data Science course in Washington, DC\n",
    "    https://github.com/justmarkham/DAT4\n",
    "    - An Introduction to Statistical Learning (James, Witten, Hastie, Tibshirani, 2013) adapted to Python code\n",
    "    https://github.com/JWarmenhoven/ISLR-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# Plots:\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.pylab as pylab\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "plt.style.use(['seaborn-white'])   \n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "              'figure.figsize': (15, 5),\n",
    "              'axes.labelsize': 'xx-large',\n",
    "              'axes.titlesize':'xx-large',\n",
    "              'xtick.labelsize':'xx-large',\n",
    "              'ytick.labelsize':'xx-large'}    \n",
    "pylab.rcParams.update(params)  #fix the parameters for the plots\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import scale  \n",
    "import sklearn.linear_model as skl_lm \n",
    "from sklearn import neighbors \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "from patsy import dmatrix \n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set: Advertising\n",
    "\n",
    "Advertising data set: \n",
    "sales of a product in relation with spending money on advertising TV, newspaper, radio in 200 different markets. \n",
    "- Response: Sales; thousands of units\n",
    "- Predictors: TV, radio, newspaper; thousands of dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "advertising = pd.read_csv('Advertising.csv', usecols=[1,2,3,4])\n",
    "advertising.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertising.head(3) #show 3 first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient estimates from data: least squares fitting\n",
    "\n",
    "Predict sales (response variable) by considering as predictor variable the TV advertisiment budget. The linear model will be: \n",
    "\n",
    "$$\n",
    "sales \\sim \\beta_0 +\\beta_1 TV\n",
    "$$\n",
    "\n",
    "We want to use our training data (predictor advertising.TV and response advertising.sales) to calculate the estimates for both regression parameters. \n",
    "\n",
    "Minimize the RSS (residual sum of squares): \n",
    "\n",
    "$$\n",
    "\\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n (y_i - \\hat{y}_i )^2 \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "y_i - \\hat{y}_i  = y_i - (\\beta_0 +\\beta_1 TV)\n",
    "$$\n",
    "\n",
    "Using the sns.regplot function we can plot directly the linear fitting that we are looking for. We want to find the two coefficients such that the resulting line is as close as possible to the 200 data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.regplot(advertising.TV, advertising.sales, order=1, ci=None, scatter_kws={'color':'r'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous plot the distances between the red points and the line are the errors made by the linear model. As the TV increase the line is less accurate. \n",
    "\n",
    "In order to calculate the model parameters (estimates of the coefficients) using least squares fitting we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression coefficients \n",
    "\n",
    "#Data: Predictor and Response\n",
    "#x_TV = scale(advertising.TV, with_mean=True, with_std=False).reshape(-1,1)#one column unknown nb of rows\n",
    "x_TV = advertising.TV.values.reshape(-1, 1)#one column unknown nb of rows\n",
    "y_sales = advertising.sales\n",
    "\n",
    "#Load the model (Ordinary Least Squares)\n",
    "regr_model = skl_lm.LinearRegression() \n",
    "\n",
    "#Fit the model with data\n",
    "regr_model.fit(x_TV,y_sales) \n",
    "\n",
    "# Coefficients estimates\n",
    "beta_0 = regr_model.intercept_\n",
    "beta_1 = regr_model.coef_[0]\n",
    "\n",
    "print('beta_0', beta_0)\n",
    "print('beta_1', beta_1)\n",
    "\n",
    "#Predict \"new\" values with the model\n",
    "x_TV_pred = np.asarray([50, 100, 200]).reshape(-1, 1)\n",
    "y_sales_pred = regr_model.predict(x_TV_pred)\n",
    "\n",
    "print('y_sales_pred', y_sales_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data non linear \n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "y = np.sin(X).ravel() #Return a contiguous flattened array.\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "# Fit regression model\n",
    "# select parameter K\n",
    "n_neighbors = 5\n",
    "\n",
    "# New samples to predict\n",
    "test = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "\n",
    "# Model\n",
    "knn_model = neighbors.KNeighborsRegressor(n_neighbors, weights='distance')\n",
    "test_knn_uniform = knn_model.fit(X, y).predict(test)\n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(8,8))\n",
    "#\n",
    "plt.scatter(X, y, c='k', label='data')\n",
    "#\n",
    "plt.scatter(test, test_knn_uniform, c='g', label='KNN uniform')\n",
    "#plt.plot(T, y_knn_uniform, c='g', linewidth=1)\n",
    "#\n",
    "plt.axis('tight')\n",
    "plt.legend()\n",
    "plt.title(\"KNN (k = \" +str(n_neighbors)+ \", weights = uniform)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN distance weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data non linear \n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "# Fit regression model\n",
    "# select parameter K\n",
    "n_neighbors = 5\n",
    "\n",
    "# New samples to predict\n",
    "test = np.linspace(0, 5, 100)[:, np.newaxis]\n",
    "\n",
    "# Model\n",
    "knn_model_d = neighbors.KNeighborsRegressor(n_neighbors, weights='distance')\n",
    "test_knn_distance = knn_model_d.fit(X, y).predict(test)\n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(8,8))\n",
    "#\n",
    "plt.scatter(X, y, c='k', label='data')\n",
    "#\n",
    "plt.scatter(test, test_knn_distance, c='r', label='KNN distance')\n",
    "#plt.plot(T, y_knn_distance, c='r')\n",
    "#\n",
    "plt.axis('tight')\n",
    "plt.legend()\n",
    "plt.title(\"KNN (k = \" +str(n_neighbors)+ \", weights = distance)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression and KNN \n",
    "\n",
    "Under which circunstances a parametric method, such as linear regression, would be better than a non-parametric, such as K-NN? \n",
    "\n",
    "Answer: The parametric one will be better if the chosen shape for the parametrization (quadratic, cubic, linear, logarithmic, etc) is close to the real fucntion $f$ that defines the real model (unknown).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal shape\n",
    "\n",
    "# Generate sample data non linear \n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "# Add noise to targets\n",
    "y[::5] += 1 * (0.5 - np.random.rand(8))\n",
    "\n",
    "# New samples to predict\n",
    "test = np.linspace(0, 5, 100)[:, np.newaxis]\n",
    "# KNN\n",
    "knn_model = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "test_knn_uniform = knn_model.fit(X, y).predict(test)\n",
    "# KNN distance\n",
    "knn_model_d = neighbors.KNeighborsRegressor(n_neighbors, weights='distance')\n",
    "test_knn_distance = knn_model_d.fit(X, y).predict(test)\n",
    "\n",
    "# Plots\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "\n",
    "# Left plot (KNN uniform)\n",
    "#\n",
    "ax1.scatter(X, y, c='k', label='data')\n",
    "#\n",
    "ax1.scatter(test, test_knn_uniform, c='g', label='KNN uniform')\n",
    "ax1.plot(test, test_knn_uniform, c='g')\n",
    "#\n",
    "sns.regplot(X, y, order=1, ci=None, scatter=False, label='Linear', ax=ax1, color='b')\n",
    "ax1.legend()\n",
    "# Right plot (KNN distance)\n",
    "#\n",
    "ax2.scatter(X, y, c='k', label='data')\n",
    "ax2.scatter(test, test_knn_distance, c='r', label='KNN distance')\n",
    "ax2.plot(test, test_knn_distance, c='r')\n",
    "#\n",
    "sns.regplot(X, y, order=1, ci=None, scatter=False, label='Linear', ax=ax2, color='b')\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear shape\n",
    "# Generate sample data linear \n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "# Add noise to targets\n",
    "y = X + np.random.rand(40,1)\n",
    "\n",
    "# Fit regression model\n",
    "n_neighbors = 5\n",
    "\n",
    "# To predict\n",
    "test = np.linspace(0, 5, 500)[:, np.newaxis]\n",
    "\n",
    "# Model\n",
    "knn_model_uniform = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "knn_model_distance = neighbors.KNeighborsRegressor(n_neighbors, weights='distance')\n",
    "test_knn_model_uniform = knn_model_uniform.fit(X, y).predict(test)\n",
    "test_knn_model_distance = knn_model_distance.fit(X, y).predict(test)\n",
    "\n",
    "# Plots\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "\n",
    "# Left plot (KNN uniform)\n",
    "#\n",
    "ax1.scatter(X, y, c='k', label='data')\n",
    "#\n",
    "ax1.scatter(test, test_knn_model_uniform, c='g', label='KNN uniform')\n",
    "ax1.plot(test, test_knn_model_uniform, c='g')\n",
    "#\n",
    "sns.regplot(X, y, order=1, ci=None, scatter=False, label='Linear', ax=ax1, color='b')\n",
    "ax1.legend()\n",
    "# Right plot (KNN distance)\n",
    "#\n",
    "ax2.scatter(X, y, c='k', label='data')\n",
    "ax2.scatter(test, test_knn_model_distance, c='r', label='KNN distance')\n",
    "ax2.plot(test, test_knn_model_distance, c='r')\n",
    "#\n",
    "sns.regplot(X, y, order=1, ci=None, scatter=False, label='Linear', ax=ax2, color='b')\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achtung!!! Overfitting risk!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient estimates from data: least squares fitting\n",
    "\n",
    "Predict sales by considering as predictor variable the TV and radio advertisiment budget. The linear model will be: \n",
    "\n",
    "$$\n",
    "sales \\sim \\beta_0 +\\beta_1 radio +\\beta_2 TV\n",
    "$$\n",
    "\n",
    "We are going to calculate the estimates coefficients as before and to plot the resulting plane considering the two predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor and response data\n",
    "x_predictors = advertising[['radio', 'TV']].values\n",
    "y_sales = advertising.sales\n",
    "\n",
    "# Load the model\n",
    "regr_model = skl_lm.LinearRegression()\n",
    "regr_model.fit(x_predictors,y_sales)\n",
    "\n",
    "\n",
    "beta_0 = regr_model.intercept_\n",
    "beta_1 = regr_model.coef_[0]\n",
    "beta_2 = regr_model.coef_[1]\n",
    "\n",
    "print('beta_0', beta_0)\n",
    "print('beta_1', beta_1)\n",
    "print('beta_2', beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the values to be predicted with the model. \n",
    "# What are the min/max values of Radio & TV?\n",
    "# Use these values to set up the grid for plotting.\n",
    "advertising[['radio', 'TV']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a coordinate grid\n",
    "Radio = np.arange(0,50) #Return evenly spaced values within a given interval.\n",
    "TV = np.arange(0,300)\n",
    "\n",
    "X1_Radio, X2_TV = np.meshgrid(Radio, TV, indexing='xy')\n",
    "Z_model = np.zeros((TV.size, Radio.size))\n",
    "\n",
    "for (i,j),v in np.ndenumerate(Z_model):\n",
    "        Z_model[i,j] =(beta_0 + X1_Radio[i,j]*beta_1 + X2_TV[i,j]*beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "fig.suptitle('Sales ~ beta_0 + beta_1 Radio + beta_2 TV Advertising', fontsize=20)\n",
    "ax = axes3d.Axes3D(fig)\n",
    "# Model\n",
    "ax.plot_surface(X1_Radio, X2_TV, Z_model, rstride=10, cstride=5, alpha=0.4)\n",
    "# Data \n",
    "ax.scatter3D(advertising.radio, advertising.TV, advertising.sales, c='r')\n",
    "\n",
    "ax.set_xlabel('Radio')\n",
    "ax.set_xlim(0,50)\n",
    "ax.set_ylabel('TV')\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_zlabel('Sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression: linear without additive assumptions\n",
    "\n",
    "Predict sales by considering as predictor variable the TV and radio advertisiment budget, but including also a relation between both quantities. \n",
    "\n",
    "The linear model will be: \n",
    "\n",
    "$$\n",
    "sales \\sim \\beta_0 +\\beta_1 radio +\\beta_2 TV + \\beta_3 radio * TV\n",
    "$$\n",
    "\n",
    "We are going to calculate the estimates coefficients as before and to plot the resulting linear plane considering the two predictor variables Radio and TV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor and response data. \n",
    "\n",
    "# 3 predictors\n",
    "x_radio = advertising[['radio']].values.reshape(-1, 1)\n",
    "x_TV = advertising[['TV']].values.reshape(-1, 1)\n",
    "x_RadxTV = np.multiply(x_radio, x_TV)\n",
    "\n",
    "# Include all in one matrix with 3 columns\n",
    "x_1 = np.concatenate((x_radio, x_TV), axis=1)\n",
    "x_predictors = np.concatenate((x_1, x_RadxTV), axis=1)\n",
    "\n",
    "# Response\n",
    "y_sales = advertising.sales\n",
    "\n",
    "# Load the model\n",
    "regr_model = skl_lm.LinearRegression()\n",
    "regr_model.fit(x_predictors,y_sales)\n",
    "\n",
    "# Coefficient estimates\n",
    "beta_0 = regr_model.intercept_\n",
    "print('beta_0', beta_0)\n",
    "\n",
    "#Vector with 3 coefficients\n",
    "beta_1 = regr_model.coef_[0]\n",
    "beta_2 = regr_model.coef_[1]\n",
    "beta_3 = regr_model.coef_[2]\n",
    "\n",
    "print('beta_1', beta_1)\n",
    "print('beta_2', beta_2)\n",
    "print('beta_3', beta_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a coordinate grid (same as before)\n",
    "#Radio = np.arange(0,50)\n",
    "#TV = np.arange(0,300)\n",
    "\n",
    "#X1_Radio, X2_TV = np.meshgrid(Radio, TV, indexing='xy')\n",
    "Z_model_mix = np.zeros((TV.size, Radio.size))\n",
    "\n",
    "for (i,j),v in np.ndenumerate(Z_model_mix):\n",
    "        Z_model_mix[i,j] =(beta_0 + X1_Radio[i,j]*beta_1 + X2_TV[i,j]*beta_2 + X1_Radio[i,j]*X2_TV[i,j]*beta_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "fig.suptitle('Sales ~ beta_0 + beta_1 Radio + beta_2 TV + beta_3 TV*Radio', fontsize=20)\n",
    "ax = axes3d.Axes3D(fig)\n",
    "\n",
    "# Model\n",
    "ax.plot_surface(X1_Radio, X2_TV, Z_model_mix, rstride=10, cstride=5, alpha=0.4)\n",
    "# Data \n",
    "ax.scatter3D(advertising.radio, advertising.TV, advertising.sales, c='r')\n",
    "\n",
    "ax.set_xlabel('Radio')\n",
    "ax.set_xlim(0,50)\n",
    "ax.set_ylabel('TV')\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_zlabel('Sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression: additive without linear assumptions\n",
    "\n",
    "## Data set: Auto\n",
    "\n",
    "Auto data set: \n",
    "Milles per galleon (mpg) of a car in relation with cylinders, weight, horsepower, etc in 392 different models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('Auto.csv', na_values='?').dropna()\n",
    "auto.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.head(3) #show 3 first rows\n",
    "# mpg: milles per galleon (response variable)\n",
    "# cylinders, displacement, horsepower, weight, acceleration (predictors variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Predict mpg by considering as predictor variable the horsepower. \n",
    "\n",
    "The linear model will be: \n",
    "\n",
    "$$\n",
    "mpg \\sim \\beta_0 +\\beta_1 horsepower\n",
    "$$\n",
    "\n",
    "We can try also a non-linear model (polynomial model of degree two):\n",
    "\n",
    "$$\n",
    "mpf \\sim \\beta_0 +\\beta_1 horsepower +\\beta_2 horsepower^2\n",
    "$$\n",
    "\n",
    "We are going to calculate the estimates coefficients as before and to plot the resulting regression line considering the predictor variable horsepower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineal regression\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "sns.regplot(auto.horsepower, auto.mpg, order=1, ci=None, scatter_kws={'color':'b'}, color='k')\n",
    "plt.title('Linear regression of mpg using horsepower value')\n",
    "plt.xlim(40,240)\n",
    "plt.ylim(5,55);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression of degree (order) 2\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "sns.regplot(auto.horsepower, auto.mpg, order=2, ci=None, scatter_kws={'color':'b'}, color='k')\n",
    "plt.title('Polynomial regression (quadratic) of mpg using horsepower value')\n",
    "plt.xlim(40,240)\n",
    "plt.ylim(5,55);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression of degree 5\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "sns.regplot(auto.horsepower, auto.mpg, order=5, ci=None, scatter_kws={'color':'b'}, color='k')\n",
    "plt.title('Polynomial regression (degree 5) of mpg using horsepower value')\n",
    "plt.xlim(40,240)\n",
    "plt.ylim(5,55);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying non-linearity using residual plots\n",
    "\n",
    "Residual plots are a useful graphical tool to identifying non-linear relationships between the predictors and the response. Given a linear regression model, \n",
    "\n",
    "* The linear model:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\hat{\\beta}_0 +\\hat{\\beta}_1 horsepower\n",
    "$$\n",
    "\n",
    "* The polynomial model of degree two (quadratic):\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\hat{\\beta}_0 +\\hat{\\beta}_1 horsepower +\\hat{\\beta}_2 horsepower^2\n",
    "$$\n",
    "\n",
    "* The residuals can be obtained: \n",
    "$$\n",
    "e_i=y_i-\\hat{y_i}\n",
    "$$\n",
    "    \n",
    "at each observed point $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include a new predictor variable\n",
    "auto['horsepower2'] = auto.horsepower**2\n",
    "auto.head(3) #show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = auto.horsepower.values.reshape(-1,1)\n",
    "y = auto.mpg\n",
    "\n",
    "# Load the model \n",
    "regr_model = skl_lm.LinearRegression()\n",
    "\n",
    "# Linear fit\n",
    "regr_model.fit(X, y)\n",
    "\n",
    "auto['pred1'] = regr_model.predict(X)\n",
    "auto['resid1'] = auto.mpg - auto.pred1\n",
    "\n",
    "# Quadratic fit\n",
    "X2 = auto[['horsepower', 'horsepower2']].values\n",
    "regr_model.fit(X2, y)\n",
    "\n",
    "auto['pred2'] = regr_model.predict(X2)\n",
    "auto['resid2'] = auto.mpg - auto.pred2\n",
    "\n",
    "auto.head(3) #show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the residuals\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "\n",
    "# Left plot (Linear regression)\n",
    "\n",
    "# Plot x-axis predicted response and y-axis residual values for each data point\n",
    "\n",
    "# Smooth fitting\n",
    "# sns.regplot with lowess: use statsmodels to estimate a nonparametric lowess model \n",
    "# (locally weighted linear regression)\n",
    "sns.regplot(auto.pred1, auto.resid1, lowess=True, \n",
    "            ax=ax1, line_kws={'color':'r', 'lw':1})\n",
    "ax1.hlines(0,\n",
    "           xmin=ax1.xaxis.get_data_interval()[0],\n",
    "           xmax=ax1.xaxis.get_data_interval()[1], \n",
    "           linestyles='dotted')\n",
    "ax1.set_title('Residual Plot for Linear Fit')\n",
    "\n",
    "# Right plot\n",
    "sns.regplot(auto.pred2, auto.resid2, lowess=True,\n",
    "            line_kws={'color':'r', 'lw':1}, ax=ax2)\n",
    "ax2.hlines(0,\n",
    "           xmin=ax2.xaxis.get_data_interval()[0],\n",
    "           xmax=ax2.xaxis.get_data_interval()[1], \n",
    "           linestyles='dotted')\n",
    "ax2.set_title('Residual Plot for Quadratic Fit')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel('$\\hat{y}_i$')\n",
    "    ax.set_ylabel('$e_i$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line is a smooth fit of the residuals, to identify visually the trend. \n",
    "* Left: a linear regression of mpg on horsepower. A strong pattern indicates non-linearity in the data. \n",
    "* Right: a linear regression of mpg on horsepower and horsepower$^2$. There is no clear pattern on the residual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic spline. Basis functions\n",
    "\n",
    "$$\n",
    "Y \\sim \\beta_0+ \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3\n",
    "$$\n",
    "\n",
    "Given data  \n",
    "$$(y_i,x_i)=(wage_i, age_i) \\mbox{ for }i=1,\\ldots,n$$\n",
    "\n",
    "The design matrix is the following:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c}y_1\\\\\\vdots\\\\y_n\\end{array}\\right]\n",
    "\\sim \n",
    "\\left[\\begin{array}{c}\n",
    "1 & x_1 & x_1^2 & x_1^3\\\\\n",
    "  & \\vdots  &\\\\\n",
    "1 & x_n & x_n^2 & x_n^3\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\\\beta_3\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cubic Spline without knots (Basis representation)\n",
    "x_grid = np.linspace(0., 1., 10) #grid points to see the matrix\n",
    "design_matrix = dmatrix(\"bs(x, df=3, degree=3)\", \n",
    "                        {\"x\": x_grid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(design_matrix)\n",
    "# [1 x x^2 x^3]\n",
    "#3+1 degrees of freedom, no knots point. Cubic polynomial.\n",
    "#print(design_matrix.design_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot B-spline basis functions (colored curves) and black line (each multiplied by the selected coeff)\n",
    "\n",
    "#Grid points to plot\n",
    "x = np.linspace(0., 1., 100)  \n",
    "design_matrix = dmatrix(\"bs(x, df=3, degree=3)\", \n",
    "                        {\"x\": x})\n",
    "\n",
    "# Select some coefficients values\n",
    "beta = np.array([0.1, 1.2, 0.1, 1]) \n",
    "\n",
    "# PLOT\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the basis functions\n",
    "plt.plot(x, design_matrix*beta); \n",
    "\n",
    "# Plot the spline itself (sum of the basis functions, thick black curve)\n",
    "plt.plot(x, np.dot(design_matrix, beta), color='k', linewidth=3);\n",
    "\n",
    "plt.title(\"Spline basis example (degree=3 and df=4)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set: Wage (Salary)\n",
    "\n",
    "Wage data set: \n",
    "Salary of a worker in relation with age, sex, etc in 3000 different persons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "wage = pd.read_csv('Wage.csv')\n",
    "print(wage.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the Wage data we are going to model the following relationship: \n",
    "* Age: predictor\n",
    "* Wage (salary): response.\n",
    "    \n",
    "    \n",
    "$$\n",
    "wage \\sim \\beta_0+ \\beta_1 age + \\beta_2 age^2 + \\beta_3 age^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# BUILD THE MODEL: \n",
    "# 1) input the data using the design matrix (PREDICTOR) and the RESPONSE\n",
    "# 2) spline regression formula, fitting\n",
    "#############################\n",
    "# Spline formula for: \n",
    "# Polynomial degree: 0 (constant), 1 (linear), 2 (quadratic), 3 (cubic)\n",
    "# Number of knots: 3 at points c_1=25, c_2=40, c_3=60\n",
    "# Degree of freedom: calculated automatically (number of columns of the design matrix)\n",
    "#############\n",
    "design_matrix_data_0 = dmatrix(\"bs(wage.age, degree=0, knots=(25,40,60), include_intercept=False)\",\n",
    "                          {\"wage.age\": wage.age}, \n",
    "                          return_type='dataframe')\n",
    "#\n",
    "design_matrix_data_1 = dmatrix(\"bs(wage.age, degree=1, knots=(25,40,60), include_intercept=False)\",\n",
    "                          {\"wage.age\": wage.age}, \n",
    "                          return_type='dataframe')\n",
    "#\n",
    "design_matrix_data_2 = dmatrix(\"bs(wage.age, degree=2, knots=(25,40,60), include_intercept=False)\",\n",
    "                          {\"wage.age\": wage.age}, \n",
    "                          return_type='dataframe')\n",
    "#\n",
    "design_matrix_data_3 = dmatrix(\"bs(wage.age, degree=3, knots=(25,40,60), include_intercept=False)\",\n",
    "                          {\"wage.age\": wage.age}, \n",
    "                          return_type='dataframe')\n",
    "#############\n",
    "regr_model_0 = sm.GLM(wage.wage, design_matrix_data_0).fit()\n",
    "regr_model_1 = sm.GLM(wage.wage, design_matrix_data_1).fit()\n",
    "regr_model_2 = sm.GLM(wage.wage, design_matrix_data_2).fit()\n",
    "regr_model_3 = sm.GLM(wage.wage, design_matrix_data_3).fit()\n",
    "#############\n",
    "# PREDICT WITH THE MODEL:\n",
    "# Use the model to predict over the new values (grid)\n",
    "#############\n",
    "age_grid = np.arange(wage.age.min(), wage.age.max()).reshape(-1,1)\n",
    "#############\n",
    "design_matrix_grid_0 = dmatrix(\"bs(age_grid, degree=0, knots=(25,40,60), include_intercept=False)\",\n",
    "                               {\"age_grid\": age_grid}, \n",
    "                               return_type='dataframe')\n",
    "design_matrix_grid_1 = dmatrix(\"bs(age_grid, degree=1, knots=(25,40,60), include_intercept=False)\",\n",
    "                               {\"age_grid\": age_grid}, \n",
    "                               return_type='dataframe')\n",
    "design_matrix_grid_2 = dmatrix(\"bs(age_grid, degree=2, knots=(25,40,60), include_intercept=False)\",\n",
    "                               {\"age_grid\": age_grid}, \n",
    "                               return_type='dataframe')\n",
    "design_matrix_grid_3 = dmatrix(\"bs(age_grid, degree=3, knots=(25,40,60), include_intercept=False)\",\n",
    "                               {\"age_grid\": age_grid}, \n",
    "                               return_type='dataframe')\n",
    "#############\n",
    "pred0 = regr_model_0.predict(design_matrix_grid_0)\n",
    "pred1 = regr_model_1.predict(design_matrix_grid_1)\n",
    "pred2 = regr_model_2.predict(design_matrix_grid_2)\n",
    "pred3 = regr_model_3.predict(design_matrix_grid_3)\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(wage.age, wage.wage, facecolor='None', edgecolor='k', alpha=0.2)\n",
    "\n",
    "plt.plot(age_grid, pred0, color='b', linewidth=3, label='Constant')\n",
    "plt.plot(age_grid, pred1, color='g', linewidth=3, label='Linear')\n",
    "plt.plot(age_grid, pred2, color='k', linewidth=3, label='Quadratic')\n",
    "plt.plot(age_grid, pred3, color='r', linewidth=3, label='Cubic')\n",
    "\n",
    "[plt.vlines(i , 0, 350, linestyles='dashed', lw=2, colors='r') for i in [25,40,60]]\n",
    "plt.legend()\n",
    "plt.xlim(15,85)\n",
    "plt.ylim(0,350)\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('wage');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients of the Cubic Spline\n",
    "print(design_matrix_data_3.shape)\n",
    "#fit3.params\n",
    "beta_0 = regr_model_3.params['Intercept']\n",
    "beta_1 = regr_model_3.params[0]\n",
    "beta_2 = regr_model_3.params[1]\n",
    "beta_3 = regr_model_3.params[2]\n",
    "beta_4 = regr_model_3.params[3]\n",
    "beta_5 = regr_model_3.params[4]\n",
    "beta_6 = regr_model_3.params[5]\n",
    "\n",
    "print(beta_0,beta_1,beta_2,beta_3,beta_4,beta_5,beta_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knots number and locations\n",
    "\n",
    "The regression spline is more flexible in regions with more knots, because the polynomial coefficients can change rapidly and adapt better the data shape. The idea it to place more knots where the function might vary rapidly and less knots where is more stable. \n",
    "\n",
    "However, in practice is common to place knots using an uniform grid. One possible way to do that is to, instead of fixing the knots, fix the degrees of freedom and then the python function will automatically place the corresponding number of knots at uniform quantiles of the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Cubic spline specifying degrees of freedom\n",
    "#############################\n",
    "# df = 1 + 3 + number of knots (python 3 + k)\n",
    "#######\n",
    "# Build the model:\n",
    "design_matrix_data_3_6 = dmatrix(\"bs(wage.age, degree=3, df=6, include_intercept=False)\",\n",
    "                        {\"wage.age\": wage.age}, \n",
    "                         return_type='dataframe')\n",
    "\n",
    "regr_model_3_6 = sm.GLM(wage.wage, design_matrix_data_3_6).fit()\n",
    "#######\n",
    "# Predict with the model:\n",
    "design_matrix_grid_3_6 = dmatrix(\"bs(age_grid, degree=3, df=6, include_intercept=False)\",\n",
    "                             {\"age_grid\": age_grid}, \n",
    "                             return_type='dataframe')\n",
    "pred3_6 = regr_model_3_6.predict(design_matrix_grid_3_6)\n",
    "#######\n",
    "print(design_matrix_grid_3_6.shape)\n",
    "#fit3_6.params\n",
    "#Python chooses 3 knots which correspond to the 25th, 50th, and 75th percentiles of age.\n",
    "#DF = Number of columns in the design matrix\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTS\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "#######\n",
    "# Left\n",
    "ax1.scatter(wage.age, wage.wage, facecolor='None', edgecolor='k', alpha=0.2)\n",
    "ax1.plot(age_grid, pred3, color='r', linewidth=3)\n",
    "[ax1.vlines(i , 0, 350, linestyles='dashed', lw=2, colors='r') for i in [25,40,60]]\n",
    "\n",
    "ax1.set_xlim(15,85)\n",
    "ax1.set_ylim(0,350)\n",
    "ax1.set_xlabel('age')\n",
    "ax1.set_ylabel('wage')\n",
    "ax1.set_title('Cubic Knots Fixed');\n",
    "#######\n",
    "# Right\n",
    "ax2.scatter(wage.age, wage.wage, facecolor='None', edgecolor='k', alpha=0.2)\n",
    "ax2.plot(age_grid, pred3_6, color='b', linewidth=3)\n",
    "[ax2.vlines(i , 0, 350, linestyles='dashed', lw=2, colors='b') for i in [33.75,42,51]]#from describe data frame\n",
    "\n",
    "ax2.set_xlim(15,85)\n",
    "ax2.set_ylim(0,350)\n",
    "ax2.set_xlabel('age')\n",
    "ax2.set_ylabel('wage')\n",
    "ax2.set_title('Cubic Df Fixed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different knots and degrees of freedom and see how the curves change. \n",
    "\n",
    "A more objective approach is to use cross-validation. Remove a portion of the data, fit a spline with the remaining data, and then use the spline to make predictions over the separated data. Repeat this process multiple times until each observation has been left out once, and then compute the overall cross-validated RSS. The procedure can be repeated with different number of knots and the value given the smallest RSS is chosen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splines vs Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Splines versus Polynomials\n",
    "#############################\n",
    "# Spline regression degree=3 df=10 (10-3 = 7 knots)\n",
    "#######\n",
    "# Fit the model\n",
    "design_matrix_data_5 = dmatrix(\"bs(wage.age, degree=3, df=10, include_intercept=False)\",\n",
    "                          {\"wage.age\": wage.age}, \n",
    "                          return_type='dataframe')\n",
    "regr_model_5 = sm.GLM(wage.wage, design_matrix_data_5).fit()\n",
    "#######\n",
    "# Predict \n",
    "age_grid = np.arange(wage.age.min(), wage.age.max()).reshape(-1,1)\n",
    "design_matrix_grid_5 = dmatrix(\"bs(age_grid, degree=3, df=10, include_intercept=False)\",\n",
    "                               {\"age_grid\": age_grid}) \n",
    "pred5 = regr_model_5.predict(design_matrix_grid_5)\n",
    "#############################\n",
    "# Plot\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(wage.age, wage.wage, facecolor='None', edgecolor='k', alpha=0.2)\n",
    "plt.plot(age_grid, pred5, color='r', linewidth=3, label='Spline')\n",
    "#######\n",
    "# Polynomial regression degree=10\n",
    "sns.regplot(wage.age, wage.wage, order=10, ci=None, scatter=False, label='Polynomial')                               \n",
    "#######                      \n",
    "plt.legend()\n",
    "plt.xlim(15,85)\n",
    "plt.ylim(0,350)\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('wage')\n",
    "plt.title('Spline (7 knots) vs Polynomial (degree 10)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set: Hitters\n",
    "Hitters data set: Records and salaries for baseball players.\n",
    "\n",
    "* baseball player’s Salary (measured in thousands of dollars)\n",
    "* Years (the number of years that he has played in the major leagues)\n",
    "* Hits (the number of hits that he made in the previous year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters = pd.read_csv('Hitters.csv', index_col=0).dropna()\n",
    "#removes all of the rows that have missing values in any variable\n",
    "hitters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Predictor Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the response and predictor variables\n",
    "# Response variable\n",
    "y_salary = hitters.Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor variables\n",
    "\n",
    "# Add dummy variables\n",
    "# League A/N\n",
    "# Division East West\n",
    "# NewLeague A/N\n",
    "dummies = pd.get_dummies(hitters[['League', 'Division', 'NewLeague']])\n",
    "#dummies.describe()\n",
    "dummies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the column with the independent variable (Salary), and change columns for which we created dummy variables\n",
    "x_predictors_ = hitters.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "# Include the new dummy variables\n",
    "x_predictors = pd.concat([x_predictors_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n",
    "#x_predictors.describe()\n",
    "x_predictors.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test \n",
    "\n",
    "We now split the samples into a TRAIN set and a TEST set in order\n",
    "to estimate later on the test error of lasso regression and select the parameter using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "# (Fct) Split arrays or matrices into random train and test subsets\n",
    "X_train, X_test , y_train, y_test = train_test_split(x_predictors, y_salary, test_size=0.5, random_state=1)\n",
    "\n",
    "# test_size: represent the proportion of the dataset to include in the test split\n",
    "# random_state: is the seed used by the random number generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso fitting method\n",
    "### Fix lasso parameter by using a uniform grid\n",
    "We will generate an array of alpha values ranging from very big to very small, essentially\n",
    "covering the full range of scenarios from the null model containing\n",
    "only the intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = 2*10**np.linspace(10,-2,100)*0.5\n",
    "print(lambdas.shape)\n",
    "print(min(lambdas))\n",
    "print(max(lambdas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associated with each alpha value is a vector of Lasso regression coefficients, which we will\n",
    "store in a matrix called `coefs`. In this case, it is a $19 \\times 100$\n",
    "matrix, with 19 rows (one for each predictor) and 100\n",
    "columns (one for each value of alpha). \n",
    "\n",
    "We will want to standardize the\n",
    "variables so that they are on the same scale. To do this, we can use the\n",
    "`normalize = True` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "regr_model_lasso = skl_lm.Lasso(max_iter=10000, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "for a in lambdas:\n",
    "    regr_model_lasso.set_params(alpha=a)\n",
    "    regr_model_lasso.fit(scale(X_train), y_train)\n",
    "    coefs.append(regr_model_lasso.coef_)\n",
    "\n",
    "#scale(X_train)\n",
    "#from sklearn.preprocessing import scale\n",
    "#Standardize a dataset along any axis\n",
    "#Center to the mean and component wise scale to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the coefficient estimates to be much smaller, in terms of $l_1$ norm,\n",
    "when a large value of alpha is used, as compared to when a small value of alpha is\n",
    "used. Let's plot and find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT\n",
    "# x axis: grid values of lambda (log scale to see something)\n",
    "# y axis \n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.gca()\n",
    "ax.plot(lambdas, coefs) \n",
    "ax.set_xscale('log') \n",
    "plt.axis('tight')\n",
    "plt.xlabel('log lambda')\n",
    "plt.ylabel('Lasso weights')\n",
    "plt.title('Lasso coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best parameter value\n",
    "Notice that in the coefficient plot that depending on the choice of the $\\lambda$\n",
    "parameter, some of the coefficients are exactly equal to zero. \n",
    "\n",
    "In order to select the best $\\lambda$ (that is, the best lasso model). We perform a K-fold cross-validation with $K=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the best Lambda Parameter (MODEL)\n",
    "# Scikit Learn sklearn.linear_model.LassoCV.\n",
    "#Lasso linear model with iterative fitting along a regularization path\n",
    "#The best model is selected by cross-validation.\n",
    "\n",
    "regr_model_lassocv = skl_lm.LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "regr_model_lassocv.fit(X_train, y_train)\n",
    "\n",
    "#alphas = None; If None alphas are set automatically\n",
    "#cv : int, integer, to specify the number of folds.\n",
    "#max_iter:  The maximum number of iterations\n",
    "#normalize = True; X will be normalized before regression by subtracting the mean and dividing by the l2-norm.\n",
    "\n",
    "#Value of the lambda parameter obtained\n",
    "print(regr_model_lassocv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the coefficients are now reduced to exactly zero.\n",
    "pd.Series(regr_model_lasso.coef_, index=x_predictors.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Mean Squared predictive error\n",
    "\n",
    "Fit the model with the TRAIN data and then test the error obtained over the TEST data.\n",
    "\n",
    "$$\n",
    "MSE=\\frac{1}{N-n}\\sum_{i=n+1}^N y_i -\\hat{y}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error with the Best lambda\n",
    "regr_model_lasso.set_params(alpha=regr_model_lassocv.alpha_)\n",
    "regr_model_lasso.fit(X_train, y_train);\n",
    "mean_squared_error(y_test, regr_model_lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error is bigger with the 1st lambda of the grid\n",
    "regr_model_lasso.set_params(alpha=lambdas[-1])\n",
    "regr_model_lasso.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, regr_model_lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error is bigger with the last lambda of the grid\n",
    "regr_model_lasso.set_params(alpha=lambdas[0])\n",
    "regr_model_lasso.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, regr_model_lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
